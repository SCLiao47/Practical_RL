{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"practice_reinforce_pytorch.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce_pytorch.ipynb","timestamp":1593570890460}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Yh2MfkCkM_eK","colab_type":"text"},"source":["# REINFORCE in PyTorch\n","\n","Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"k5NokaLUM_eL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1593571335747,"user_tz":240,"elapsed":15303,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}},"outputId":"a1da6ed4-6fd0-4866-8cef-9b093d058e8c"},"source":["import sys, os\n","if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","    !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Selecting previously unselected package xvfb.\n","(Reading database ... 144379 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Starting virtual X frame buffer: Xvfb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KnsF7KZZM_eO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593571335833,"user_tz":240,"elapsed":13381,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QnMTbOMDM_eS","colab_type":"text"},"source":["A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."]},{"cell_type":"code","metadata":{"id":"DYO-8LSuM_eS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1593571340629,"user_tz":240,"elapsed":1947,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}},"outputId":"c20aa832-36ca-45f6-cabb-9a918a9d9403"},"source":["env = gym.make(\"CartPole-v0\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fb5fbb749b0>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"tRGe4xmbQ-A5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593571549302,"user_tz":240,"elapsed":230,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}},"outputId":"5d8cfa90-2c3f-4f1b-e510-ebc77845c44c"},"source":["print(n_actions)\n","print(state_dim)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["2\n","(4,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mID6OkghM_eV","colab_type":"text"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"UMmoNotQM_eV","colab_type":"text"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__.\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"twZd1ky2M_eW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593571373697,"user_tz":240,"elapsed":2370,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["import torch\n","import torch.nn as nn"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqiu__WlM_eY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593577449872,"user_tz":240,"elapsed":333,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["# Build a simple neural network that predicts policy logits. \n","# Keep it simple: CartPole isn't worth deep architectures.\n","model = nn.Sequential(\n","  # <YOUR CODE: define a neural network that predicts policy logits>\n","    nn.Linear(state_dim[0], 32),\n","    nn.ReLU(),\n","    nn.Linear(32,32),\n","    nn.ReLU(),\n","    nn.Linear(32,n_actions)\n","\n",")"],"execution_count":58,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rq_PaGQ8M_ea","colab_type":"text"},"source":["#### Predict function"]},{"cell_type":"markdown","metadata":{"id":"bN4tiXpVM_ea","colab_type":"text"},"source":["Note: output value of this function is not a torch tensor, it's a numpy array.\n","So, here gradient calculation is not needed.\n","<br>\n","Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n","to suppress gradient calculation.\n","<br>\n","Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n","<br>\n","With `.detach()` computational graph is built but then disconnected from a particular tensor,\n","so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n","<br>\n","In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."]},{"cell_type":"code","metadata":{"id":"6O4AylGjM_eb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593577451108,"user_tz":240,"elapsed":314,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    # convert states, compute logits, use softmax to get probability\n","    # <YOUR CODE>\n","    # return <YOUR CODE>\n","    states = torch.tensor(states, dtype=torch.float32)\n","    pi = model(states)\n","    \n","    # print(pi)\n","\n","    with torch.no_grad():\n","        out = nn.functional.softmax(pi,dim=1)\n","\n","    return out.numpy()"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"EooZFTXDM_ed","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593577451204,"user_tz":240,"elapsed":178,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["test_states = np.array([env.reset() for _ in range(5)])\n","test_probas = predict_probs(test_states)\n","assert isinstance(test_probas, np.ndarray), \\\n","    \"you must return np array and not %s\" % type(test_probas)\n","assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n","    \"wrong output shape: %s\" % np.shape(test_probas)\n","assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\"\n","\n","# print(test_probas)"],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4NmRsMvM_ef","colab_type":"text"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"3Yrk4CX3M_eg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593577452022,"user_tz":240,"elapsed":405,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    Play a full session with REINFORCE agent.\n","    Returns sequences of states, actions, and rewards.\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(np.array([s]))[0]\n","\n","        # Sample action with given probabilities.\n","        # a = <YOUR CODE>\n","        a = np.random.choice(2, p=action_probs)\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"3WuFuLEwM_ei","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593577452222,"user_tz":240,"elapsed":386,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["# test it\n","states, actions, rewards = generate_session(env)"],"execution_count":62,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ssq_xy_uM_ek","colab_type":"text"},"source":["### Computing cumulative rewards\n","\n","$$\n","\\begin{align*}\n","G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n","&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n","&= r_t + \\gamma * G_{t + 1}\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"-u87Y0SqM_ek","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593577452651,"user_tz":240,"elapsed":318,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    # <YOUR CODE>\n","    # return <YOUR CODE: array of cumulative rewards>\n","    G = rewards.copy()\n","\n","    for idx in range(len(G)-2,-1,-1):\n","        G[idx] = rewards[idx] + gamma*G[idx+1]\n","        \n","    return G"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"busKVHRjM_em","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593577452782,"user_tz":240,"elapsed":179,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}},"outputId":"01f21ded-023d-49f0-c332-e9b142114024"},"source":["get_cumulative_rewards(rewards)\n","assert len(get_cumulative_rewards(list(range(100)))) == 100\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","    [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":64,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jj3Nu0mYM_ep","colab_type":"text"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n","\n","$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n","\n","$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n","\n","\n","#### My Notes\n","Entropy for action is\n","\n","$$ H(\\pi(\\cdot|s_t)) = - \\sum_{a\\in A} \\pi(a|s_t)\\log{\\pi(a|s_t)}$$\n","\n","which is high if a policy choose more evenly among the available actions.\n","\n","Including objective function $\\hat{J}$ and entorpy $H$ into loss for gradient ascent, the agent optimizes the objective while being encouraged exploration. "]},{"cell_type":"code","metadata":{"id":"tdJpWJFLM_ep","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593577453387,"user_tz":240,"elapsed":255,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["def to_one_hot(y_tensor, ndims):\n","    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n","    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n","    y_one_hot = torch.zeros(\n","        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n","    return y_one_hot"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"XIrxOodGM_er","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593577453795,"user_tz":240,"elapsed":273,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["# Your code: define optimizers\n","optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n","\n","\n","def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n","    \"\"\"\n","    Takes a sequence of states, actions and rewards produced by generate_session.\n","    Updates agent's weights by following the policy gradient above.\n","    Please use Adam optimizer with default parameters.\n","    \"\"\"\n","\n","    # cast everything into torch tensors\n","    states = torch.tensor(states, dtype=torch.float32)\n","    actions = torch.tensor(actions, dtype=torch.int32)\n","    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n","    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n","\n","    # predict logits, probas and log-probas using an agent.\n","    logits = model(states)\n","    probs = nn.functional.softmax(logits, -1)\n","    log_probs = nn.functional.log_softmax(logits, -1)\n","\n","    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n","        \"please use compute using torch tensors and don't use predict_probs function\"\n","\n","    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n","    log_probs_for_actions = torch.sum(\n","        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n","   \n","    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n","    entropy = torch.sum(probs*log_probs)\n","    J_hat = torch.mean(log_probs_for_actions*cumulative_returns)\n","\n","    loss = -(J_hat + entropy_coef*entropy)\n","\n","    # Gradient descent step\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # technical: return session rewards to print them later\n","    return np.sum(rewards)"],"execution_count":66,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2rGCGQxBM_et","colab_type":"text"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"id":"8ofAn8H5M_et","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1593577502263,"user_tz":240,"elapsed":47178,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}},"outputId":"c2bc4170-fc86-4f89-f9a4-3dbae82f4c8b"},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n","    \n","    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n","    \n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":67,"outputs":[{"output_type":"stream","text":["mean reward:21.410\n","mean reward:23.380\n","mean reward:26.530\n","mean reward:34.920\n","mean reward:54.030\n","mean reward:82.500\n","mean reward:183.940\n","mean reward:251.850\n","mean reward:242.320\n","mean reward:121.090\n","mean reward:181.970\n","mean reward:617.180\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a-6aa7fYM_ew","colab_type":"text"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"fdspwfagM_ew","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593577525958,"user_tz":240,"elapsed":17541,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}}},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"0AM5QejmM_ey","colab_type":"code","colab":{"resources":{"http://localhost:8080/videos/openaigym.video.0.121.video000064.mp4":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""}},"base_uri":"https://localhost:8080/","height":501},"executionInfo":{"status":"ok","timestamp":1593577545722,"user_tz":240,"elapsed":304,"user":{"displayName":"廖士齊","photoUrl":"","userId":"15119494860324039567"}},"outputId":"045401ba-99a7-40e1-88ab-fbc5ff0aa176"},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"videos/openaigym.video.0.121.video000064.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"coT-6ylxM_e0","colab_type":"code","colab":{}},"source":["from submit import submit_cartpole\n","submit_cartpole(generate_session, 'your.email@example.com', 'YourAssignmentToken')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"msjZowfPM_e2","colab_type":"text"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}]}